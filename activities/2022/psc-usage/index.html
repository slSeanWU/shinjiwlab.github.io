<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>WAVLab | PSC Usage</title>
<meta name="description" content="Webpage of Watanabe's Audio and Voice (WAV) Lab
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="/assets/img/favicon.png">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/activities/2022/psc-usage/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       WAVLab
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              About
              
            </a>
          </li>

          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/members/">
                Members
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                Publications
                
              </a>
          </li>
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/open_source">
                Open-source
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/courses/">
                Courses
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sponsors/">
                Sponsors
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/info/">
                Info
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/positions/">
                Positions
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/sphinx_lunch">
                Sphinx Lunch
                
              </a>
          </li>
          
          
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              Activities
              
            </a>
          </li>
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      

<div class="post">

  <header class="post-header">
    <h1 class="post-title">PSC Usage</h1>
    <p class="post-meta">January 1, 2022</p>
  </header>

  <article class="post-content">
    <h1 id="misc-resources">Misc. resources</h1>
<ul>
  <li>Bridges-2 manual: https://www.psc.edu/resources/bridges-2/user-guide-2/</li>
  <li>Connect using browser: https://ondemand.bridges2.psc.edu/ (optional)</li>
  <li>ESPnet installation guide (general purpose): https://espnet.github.io/espnet/installation.html</li>
</ul>

<h1 id="summary-of-psc-usage-and-the-partitions">Summary of PSC usage and the partitions</h1>
<ul>
  <li>PSC has limited service units (SUs) for GPU and RM (Regular Memory) resource availability.</li>
  <li><code class="language-plaintext highlighter-rouge">sinfo</code> lists all the available partitions in PSC and their status.
    <ul>
      <li>Partitions: GPU, GPU-shared, RM-small, RM, RM-512, RM-shared</li>
    </ul>
  </li>
  <li>Please submit jobs by specifying the partitions clearly
    <ul>
      <li>Running GPU jobs: <code class="language-plaintext highlighter-rouge">GPU</code>, <code class="language-plaintext highlighter-rouge">GPU-shared</code></li>
      <li>Running CPU jobs: <code class="language-plaintext highlighter-rouge">RM-small</code>, <code class="language-plaintext highlighter-rouge">RM</code>, <code class="language-plaintext highlighter-rouge">RM-512</code>, <code class="language-plaintext highlighter-rouge">RM-shared</code></li>
      <li><strong>Default partition</strong> is <code class="language-plaintext highlighter-rouge">RM</code>, which request to allocate a 128-cpu node. (<strong>Be careful of this case</strong>)</li>
    </ul>
  </li>
</ul>

<h2 id="account-creation">Account creation</h2>
<ul>
  <li>Create an account for <a href="https://portal.xsede.org/my-xsede#/guest">PSC</a></li>
  <li>Send the username to allocation managers (e.g. Xuankai) to add the user in our group.</li>
</ul>

<h2 id="login">login</h2>
<ul>
  <li>check https://portal.xsede.org/web/xup/single-sign-on-hub
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ssh -l XUPusername login.xsede.org
[XUPusername@ssohub ~]$ gsissh bridges2
[XUPusername@bridges2-login013 shared]$
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="gpu-partitions">GPU Partitions</h2>
<ul>
  <li>In <code class="language-plaintext highlighter-rouge">GPU / GPU-shared</code> partitions, each node consists of 8 v100 GPU devices</li>
  <li>There are two types of GPU nodes: <code class="language-plaintext highlighter-rouge">v100-16</code> and <code class="language-plaintext highlighter-rouge">v100-32</code> having GPU units with 16GB and 32GB memory respectively.</li>
  <li>Submit jobs to <code class="language-plaintext highlighter-rouge">GPU-shared</code> partition. (<strong>suggested</strong>)
    <ul>
      <li>Using <code class="language-plaintext highlighter-rouge">-p GPU-shared --gpus=type:n</code> in <code class="language-plaintext highlighter-rouge">sbatch</code> or <code class="language-plaintext highlighter-rouge">srun</code>. Here <code class="language-plaintext highlighter-rouge">type</code> can be <code class="language-plaintext highlighter-rouge">v100-16</code> or <code class="language-plaintext highlighter-rouge">v100-32</code> and <code class="language-plaintext highlighter-rouge">n</code> can range from 1 to 4.</li>
    </ul>
  </li>
  <li>Submit jobs to <code class="language-plaintext highlighter-rouge">GPU</code> partition.
    <ul>
      <li><strong>Please use it only when necessary.</strong></li>
      <li>Using <code class="language-plaintext highlighter-rouge">-p GPU</code> in <code class="language-plaintext highlighter-rouge">sbatch</code> or <code class="language-plaintext highlighter-rouge">srun</code>. It request to allocate a whole GPU node, 8 GPUs, for each job.</li>
      <li>In this case, it deducts 8 SUs from our team’s GPU allocation every hour the job runs.</li>
    </ul>
  </li>
</ul>

<h2 id="rm-partitions">RM Partitions</h2>
<ul>
  <li>In the <code class="language-plaintext highlighter-rouge">RM and RM-512</code> partitions, each node consists of 128 cores.</li>
  <li>Nodes in RM, RM-shared partitions have a memory of 128GB, while nodes in RM-512 partitions have 512GB memory.</li>
  <li>using an entire node for an hour will deduct 128 SUs from our team’s Regular Memory allocation.</li>
  <li>Submit jobs to <code class="language-plaintext highlighter-rouge">RM-shared</code> partition. (<strong>suggested</strong>)
    <ul>
      <li>Using <code class="language-plaintext highlighter-rouge">-p RM-shared --ntasks-per-node=n</code> in <code class="language-plaintext highlighter-rouge">sbatch</code> or <code class="language-plaintext highlighter-rouge">srun</code>. Here <code class="language-plaintext highlighter-rouge">n</code> can range from 1 to 64.</li>
      <li>Usually, jobs only require a few cpu cores.</li>
    </ul>
  </li>
  <li>Submit jobs to <code class="language-plaintext highlighter-rouge">RM</code> partition.
    <ul>
      <li><strong>Please use it only when necessary.</strong></li>
      <li>It request to allocate all of the 128 cpu cores.</li>
      <li>Using <code class="language-plaintext highlighter-rouge">-p RM --ntasks-per-node=n</code> in <code class="language-plaintext highlighter-rouge">sbatch</code> or <code class="language-plaintext highlighter-rouge">srun</code>. Here <code class="language-plaintext highlighter-rouge">n</code> can range from 1 to 64.</li>
    </ul>
  </li>
</ul>

<h2 id="other-usage">Other usage</h2>
<ul>
  <li>Data copying / file transfer
    <ul>
      <li>Suggest to use <code class="language-plaintext highlighter-rouge">data.bridges2.psc.edu</code> as the machine name. Doing file transfer with <code class="language-plaintext highlighter-rouge">rsync</code>, <code class="language-plaintext highlighter-rouge">sftp</code>, <code class="language-plaintext highlighter-rouge">scp</code>, etc. <a href="https://portal.xsede.org/psc-bridges-2#filetransfer-copybridgesfiles">Detailed usage info</a>. Following is an example of <code class="language-plaintext highlighter-rouge">scp</code>.
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  # This requires the enrollment of Two-Factor Authentication (TFA)
  scp -P 2222 myfile XSEDE-username@data.bridges2.psc.edu:/path/to/file
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Submitting jobs with dependency
    <ul>
      <li>This can be used to submit a job which is expected to start run after some specific jobs finish. In many cases, training a model can take a few days. However, PSC has the restriction that each job can run for 2 days at most. In this case, we can start a job with dependency for long jobs. For example, you already start a job ID is 000001 and you want a following job right after it. You can submit jobs like:
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  sbatch --time 2-0:00:00 --dependency=afterany:000001 run.sh
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Common arguments in sbatch / srun
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>-p, --partition=partition               partition requested
-J, --job-name=jobname                  name of job
-t, --time=time                         time limit
--gres=rsrc_name[:rsrc_type]:rsrc_num   required generic resources
-c, --cpus-per-task=ncpus               number of cpus required per task
-d, --dependency=type:jobid[:time]      defer job until condition on jobid is satisfied
-e, --error=err                         file for batch script's standard error
-o, --output=out                        file for batch script's standard output
--mem-per-cpu=MB                        maximum amount of real memory per allocated
--ntasks-per-node=n                     number of tasks to invoke on each node
--reservation=name                      allocate resources from named reservation, e.g. `GPUcis210027`
</code></pre></div>    </div>
  </li>
  <li>View tools
    <ul>
      <li>slurm commands
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># view jobs in the queue
squeue -u ${username}
     
# view detailed job info
scontrol show jobid -d ${jobid}
     
# view job history and billing info, e.g. since time 04/22/2022 12am.
sacct -u ${username} -S 2022-04-22T00:00:00 --format=JobID,jobname,user,elapsed,nnodes,alloccpus,state,partition,nodelist,AllocTRES%50,CPUTime 
</code></pre></div>        </div>
      </li>
      <li>PSC provides <code class="language-plaintext highlighter-rouge">slurm-tool</code>
        <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># Show or watch job queue:
slurm-tool [watch] queue     show own jobs
slurm-tool [watch] q &lt;user&gt;  show user's jobs
slurm-tool [watch] quick     show quick overview of own jobs
slurm-tool [watch] shorter   sort and compact entire queue by job size
slurm-tool [watch] short     sort and compact entire queue by priority
slurm-tool [watch] full      show everything
slurm-tool [w] [q|qq|ss|s|f] shorthands for above!

slurm-tool qos               show job service classes
slurm-tool top [queue|all]   show summary of active users

# Show detailed information about jobs:
slurm-tool prio [all|short]  show priority components
slurm-tool j|job &lt;jobid&gt;     show everything else
slurm-tool steps &lt;jobid&gt;     show memory usage of running srun job steps

# Show usage and fair-share values from accounting database:
slurm-tool h|history &lt;time&gt;  show jobs finished since, e.g. "1day" (default)
slurm-tool shares

# Show nodes and resources in the cluster:
slurm-tool p|partitions      all partitions
slurm-tool n|nodes           all cluster nodes
slurm-tool c|cpus            total cpu cores in use
slurm-tool cpus &lt;partition&gt;  cores available to partition, allocated and free
slurm-tool cpus jobs         cores/memory reserved by running jobs
slurm-tool cpus queue        cores/memory required by pending jobs
slurm-tool features          List features and GRES

# Other:
slurm-tool v|version         Print versions of slurm-tool tool and slurm itself
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>

<h1 id="important">Important</h1>
<ul>
  <li><code class="language-plaintext highlighter-rouge">Home</code> directory is of limited space. Please do most of your work in ocean storage (<code class="language-plaintext highlighter-rouge">$ cd ${PROJECT}</code>)</li>
  <li>When you publish a paper, please acknowledge the PSC. We will get benefit when we apply for PSC credits next time.
    <ul>
      <li><a href="https://www.psc.edu/resources/bridges/acknowledgement-in-publications/">Acknowledgement webpage</a>
        <ul>
          <li>Example: This work used the Extreme Science and Engineering Discovery Environment (XSEDE) ~\cite{ecss}, which is supported by National Science Foundation grant number ACI-1548562. Specifically, it used the Bridges system ~\cite{nystrom2015bridges}, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@ARTICLE{xsede,
author = {J. Towns and T. Cockerill and M. Dahan and I. Foster and K. Gaither and A. Grimshaw and V. Hazlewood and S. Lathrop and D. Lifka and G. D. Peterson and R. Roskies and J. R. Scott and N. Wilkins-Diehr},
journal = {Computing in Science \&amp; Engineering},
title = {XSEDE: Accelerating Scientific Discovery},
year = {2014},
volume = {16},
number = {5},
pages = {62-74},
keywords={Knowledge discovery;Scientific computing;Digital systems;Materials engineering;Supercomputers},
doi = {10.1109/MCSE.2014.80},
url = {doi.ieeecomputersociety.org/10.1109/MCSE.2014.80},
ISSN = {1521-9615},
month={Sept.-Oct.}
}
@inproceedings{nystrom2015bridges,
  title={Bridges: a uniquely flexible HPC resource for new communities and data analytics},
  author={Nystrom, Nicholas A and Levine, Michael J and Roskies, Ralph Z and Scott, J Ray},
  booktitle={Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure},
  pages={1--8},
  year={2015}
}
</code></pre></div></div>

<h1 id="espnet-installation-steps">ESPnet installation steps</h1>
<ol>
  <li>Miniconda installation
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
bash Miniconda3-latest-Linux-x86_64.sh
</code></pre></div>    </div>
  </li>
  <li>Load modules and set environment
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>module load cuda/10.2.0 cudnn mkl
<span class="nv">CUDAROOT</span><span class="o">=</span>/jet/packages/spack/opt/spack/linux-centos8-zen/gcc-8.3.1/cuda-10.2.89-kz7u4ix6ed53nioz4ycqin3kujcim3bs
</code></pre></div>    </div>
  </li>
  <li>Kaldi installation:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;
git clone https://github.com/kaldi-asr/kaldi
<span class="nb">cd </span>kaldi/tools
make <span class="nt">-j</span> 8
./extras/install_irstlm.sh
<span class="nb">cd</span> ../src/
./configure <span class="nt">--use-cuda</span><span class="o">=</span>no
make <span class="nt">-j</span> clean depend<span class="p">;</span> make <span class="nt">-j</span> 8
</code></pre></div>    </div>
  </li>
  <li>ESPNet installation:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;
git clone https://github.com/espnet/espnet
<span class="nb">cd </span>espnet/tools/
<span class="nb">ln</span> <span class="nt">-s</span> /ocean/projects/cis210027p/&lt;user&gt;/kaldi <span class="nb">.</span>
<span class="nb">.</span> ./setup_cuda_env.sh <span class="k">${</span><span class="nv">CUDAROOT</span><span class="k">}</span>
./setup_venv.sh <span class="si">$(</span><span class="nb">command</span> <span class="nt">-v</span> python3<span class="si">)</span>
make <span class="nt">-j</span> 8 <span class="nv">CUDA_VERSION</span><span class="o">=</span>10.2 <span class="nv">TH_VERSION</span><span class="o">=</span>1.8.1
</code></pre></div>    </div>
  </li>
  <li>Verify torch installation:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun <span class="nt">--pty</span> <span class="nt">-p</span> GPU-shared <span class="nt">-N</span> 1 <span class="nt">--gpus</span><span class="o">=</span>v100-16:1 /bin/bash <span class="nt">-l</span>
nvidia-smi
<span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;/espnet/tools
<span class="nb">.</span> ./activate_python.sh
python <span class="nt">-c</span> <span class="s2">"import torch; print(torch.cuda.is_available())"</span>
<span class="nb">exit</span>
</code></pre></div>    </div>
  </li>
</ol>

<h1 id="espnet-usage-tutorial">ESPnet usage tutorial</h1>
<ul>
  <li>Full documentation: https://espnet.github.io/espnet/tutorial.html</li>
</ul>

<h2 id="running-an-example-an4-recipe-with-runsh-using-slurm-backend">Running an example <code class="language-plaintext highlighter-rouge">an4</code> recipe with <code class="language-plaintext highlighter-rouge">run.sh</code> using slurm backend</h2>
<ol>
  <li>Go the the directory: <code class="language-plaintext highlighter-rouge">cd egs2/an4/asr1</code></li>
  <li>Modify <code class="language-plaintext highlighter-rouge">cmd.sh</code> and <code class="language-plaintext highlighter-rouge">conf/slurm.conf</code> to be ready for slurm management
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cmd.sh</span>

<span class="c"># line 31</span>
<span class="nv">cmd_backend</span><span class="o">=</span><span class="s1">'slurm'</span>  <span class="c"># cmd_backend='local'</span>
</code></pre></div>    </div>
    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># conf/slurm.conf</span>

<span class="c1"># Default configuration                                                                                                                                                        </span>
<span class="s">command sbatch --export=PATH</span>
<span class="s">option name=* --job-name $0</span>
<span class="s">default time=48:00:00</span>
<span class="s">option time=* --time $0</span>
<span class="s">option mem=* --mem-per-cpu $0</span>
<span class="s">option mem=0</span>
<span class="s">option num_threads=* --cpus-per-task $0</span>
<span class="s">option num_threads=1 --cpus-per-task </span><span class="m">1</span>
<span class="s">option num_nodes=* --nodes $0</span>
<span class="s">default gpu=0</span>
<span class="s">option gpu=0 -p RM-shared</span>
<span class="s">option gpu=* -p GPU-shared --gres=gpu:$0 -c $0</span>  <span class="c1"># Recommend allocating more CPU than, or equal to the number of GPU</span>
<span class="c1"># note: the --max-jobs-run option is supported as a special case</span>
<span class="c1"># by slurm.pl and you don't have to handle it in the config file.</span>
</code></pre></div>    </div>
  </li>
  <li>Run experiments
    <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./run.sh --stage 1 --stop-stage 13
</code></pre></div>    </div>
  </li>
</ol>

<h2 id="running-an-example-an4-recipe-step-by-step">Running an example <code class="language-plaintext highlighter-rouge">an4</code> recipe step by step:</h2>
<ol>
  <li>Execute the data preparation stages: <code class="language-plaintext highlighter-rouge">-1</code> and <code class="language-plaintext highlighter-rouge">0</code>
    <ul>
      <li>Stage <code class="language-plaintext highlighter-rouge">-1</code> downloads and un-tars the <code class="language-plaintext highlighter-rouge">an4</code> dataset with 948 training and 130 test utterances.</li>
      <li>Stage <code class="language-plaintext highlighter-rouge">0</code> prepares the dataset by creating <code class="language-plaintext highlighter-rouge">data/train</code> and <code class="language-plaintext highlighter-rouge">data/test</code> directories. Names of these directories can vary for other datasets. 
Each of these directories contains 4 files: <code class="language-plaintext highlighter-rouge">wav.scp</code>, <code class="language-plaintext highlighter-rouge">text</code>, <code class="language-plaintext highlighter-rouge">utt2spk</code> and <code class="language-plaintext highlighter-rouge">spk2utt</code>. 
A mapping from a unique utterance-ID to the utterance’s filepath, text and speaker-ID is noted in <code class="language-plaintext highlighter-rouge">wav.scp</code> , <code class="language-plaintext highlighter-rouge">text</code>, <code class="language-plaintext highlighter-rouge">utt2spk</code> files respectively. An inverse mapping from the speaker-ID to the speaker’s utterance-IDs is noted in <code class="language-plaintext highlighter-rouge">spk2utt file</code>.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;/espnet
<span class="nb">cd </span>egs/an4/asr1
./run.sh <span class="nt">--stage</span> <span class="nt">-1</span> <span class="nt">--stop_stage</span> 0
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Execute the feature extraction stage: <code class="language-plaintext highlighter-rouge">1</code>
    <ul>
      <li>Stage <code class="language-plaintext highlighter-rouge">1</code> extracts the 80 log-mel and 3 pitch features from a 25ms frame shifted every 10ms for each audio sample.</li>
      <li>Parameter <code class="language-plaintext highlighter-rouge">nj</code> in this stage’s code represents the number of CPUs used to parallelly extract the features.</li>
      <li>Since this dataset doesn’t provide a validation split, we split the 948 training utterances into 848 training and 100 dev samples.</li>
      <li>A mapping from utterance-IDs to feature filepaths is noted in <code class="language-plaintext highlighter-rouge">data/*/feats.scp</code> file and extracted features are stored in the <code class="language-plaintext highlighter-rouge">dump/*/deltafalse/feats.{1-$nj}.ark</code> files.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./run.sh <span class="nt">--stage</span> 1 <span class="nt">--stop_stage</span> 1
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Prepare a dictionary and data.json in stage: <code class="language-plaintext highlighter-rouge">2</code>
    <ul>
      <li>A mapping of each character, special tokens (ex: <unk>,<space> etc.) to a unique token-ID is stored as a dictionary at `data/lang_1char/train_nodev_units.txt`</space></unk></li>
      <li>Pairs of extracted features and mapped tokens (using above dictionary) for each utterance-ID is stored as a JSON file in the respective <code class="language-plaintext highlighter-rouge">dump</code> directories.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>./run.sh <span class="nt">--stage</span> 2 <span class="nt">--stop_stage</span> 2
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Train the RNN-LM and ASR models in stages: <code class="language-plaintext highlighter-rouge">3</code> and <code class="language-plaintext highlighter-rouge">4</code>
    <ul>
      <li>Trained LM models are stored by default in <code class="language-plaintext highlighter-rouge">exp/train_rnnlm_pytorch_lm_word100/</code> directory</li>
      <li>Trained ASR models are stored by default in <code class="language-plaintext highlighter-rouge">exp/train_nodev_pytorch_train_mtlalpha1.0/results/</code> directory</li>
      <li>Request GPU resources for training:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch <span class="nt">-t</span> 2-00:00:00 <span class="nt">-p</span> GPU-shared <span class="nt">-N</span> 1 <span class="nt">--gpus</span><span class="o">=</span>v100-16:1 <span class="nt">--mem</span><span class="o">=</span>16G train_model.sh
</code></pre></div>        </div>
      </li>
      <li>Preview of <code class="language-plaintext highlighter-rouge">train_model.sh</code> file to train the RNN based language model (RNN-LM) in stage: <code class="language-plaintext highlighter-rouge">3</code>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
module load cuda/10.2.0 cudnn mkl
<span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;/espnet/egs/an4/asr1
./run.sh <span class="nt">--stage</span> 3 <span class="nt">--stop_stage</span> 3
</code></pre></div>        </div>
      </li>
      <li>Preview of <code class="language-plaintext highlighter-rouge">train_model.sh</code> file to train the ASR model in stage: <code class="language-plaintext highlighter-rouge">4</code>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
module load cuda/10.2.0 cudnn mkl
<span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;/espnet/egs/an4/asr1
./run.sh <span class="nt">--stage</span> 4 <span class="nt">--stop_stage</span> 4
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Request computational resources and perform decoding in stage: <code class="language-plaintext highlighter-rouge">5</code>
    <ul>
      <li>Parameter <code class="language-plaintext highlighter-rouge">nj</code> in this stage’s code represents the number of CPUs used to parallelly decode each recognition set: validation, test splits. So we must request 2x<code class="language-plaintext highlighter-rouge">nj</code> number of CPUs in total.</li>
      <li>Note that for each CPU, we can request a maximum memory of 2000M.</li>
      <li>Request RM-shared node resources for decoding:
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>sbatch <span class="nt">-t</span> 12:00:00 <span class="nt">-p</span> RM-shared <span class="nt">-N</span> 1 <span class="nt">--cpus-per-task</span><span class="o">=</span>16 <span class="nt">--mem</span><span class="o">=</span>32000M decode_model.sh
</code></pre></div>        </div>
      </li>
      <li>Preview of <code class="language-plaintext highlighter-rouge">decode_model.sh</code> file to decode the ASR model in stage: <code class="language-plaintext highlighter-rouge">5</code>
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/usr/bin/env bash</span>
<span class="nb">cd</span> /ocean/projects/cis210027p/&lt;user&gt;/espnet/egs/an4/asr1
./run.sh <span class="nt">--stage</span> 5 <span class="nt">--stop_stage</span> 5
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>Misc.
    <ul>
      <li>An interrupted training can be resumed by specifying <code class="language-plaintext highlighter-rouge">--resume</code> parameter with the path to most recent snapshot in <code class="language-plaintext highlighter-rouge">exp/train_nodev_pytorch_train_mtlalpha1.0/results/</code> directory.</li>
      <li>A unique tag for managing your experiments can be set by specifying <code class="language-plaintext highlighter-rouge">--lmtag</code> and <code class="language-plaintext highlighter-rouge">--tag</code> parameters.</li>
      <li>Multi-GPU training can be done by specifying <code class="language-plaintext highlighter-rouge">--ngpu</code> in the training stages.</li>
      <li>For interactive debugging purposes, <code class="language-plaintext highlighter-rouge">srun</code> command can be used to request GPU-shared or RM-shared nodes as below. Note that PSC has limited service units (SUs), so use <code class="language-plaintext highlighter-rouge">srun</code> based debugging for only the required duration.
        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>srun <span class="nt">--pty</span> <span class="nt">-p</span> GPU-shared <span class="nt">-N</span> 1 <span class="nt">--gpus</span><span class="o">=</span>v100-16:1 /bin/bash <span class="nt">-l</span>
srun <span class="nt">--pty</span> <span class="nt">-p</span> RM-shared <span class="nt">-N</span> 1 <span class="nt">--cpus-per-task</span><span class="o">=</span>16 <span class="nt">--mem</span><span class="o">=</span>32000M /bin/bash <span class="nt">-l</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ol>

  </article>

  

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2022   WAV Lab.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
